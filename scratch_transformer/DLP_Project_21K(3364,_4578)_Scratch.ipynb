{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpzsBH-oIZ0U",
        "outputId": "36186c27-df63-46ce-c748-b34221ec0c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "b0ad51dcee7140c89d2ded5cea02bb97",
            "1cba4f75d60340d186b184b03e61b5cc",
            "4be46c5d63b440be874bcabbd3aad777",
            "18315fe95ea840ea9ad842f8e04f2fb9",
            "87f47d083acb484c829de79191b29af3",
            "3f648acc8ea74aadb24dd3680f28a63d",
            "3df996eb470f43fd840f4e7ef7f8f423",
            "8df689d6a61349f480c8ddbfc1b12202",
            "16c882338d4f4facb47ddf1e4d5bef57",
            "fc30ad71e2c240abb319438d9c785168",
            "374089da8a8b4913a5708db903875fe9",
            "7dc2a3d59da94d448308ad938f703c98",
            "1e4b608a58bd4b44a528768126a42bbd",
            "3aeec118061a4ed9b8932625d0309c20",
            "e48547de62f54d07ba4e224d98a392da",
            "37aec77be48146999c155177fa7e0caf",
            "ab134096846b446ca199d2334f469536",
            "bead121e2d9d4a3db2532a4cfc24267b",
            "9b7dd395345645edbd3f529ca2164a34",
            "8eaa770fb50f4717ad9b2b54f3e1e0a2",
            "941a8e51c9f644ae8ae4c6cb56e89bca",
            "bbcdc9e04bdb470984ae31a7e1d157ae",
            "dd338cccbc9a474b859994a8285b3be7",
            "49fcee6879104634a9ef5938fd173594",
            "9e6d8c267348463fab6540119a855c0d",
            "532a5027a26d481796abed5beb7686de",
            "1083f2467b0546e1bb3f767fe24e048f",
            "8e457105ae614e7c83ab381a00a3e187",
            "57afb409d15c47739aee2906d51bed7c",
            "e21dcddb5b2a451d81123ff333ac2dec",
            "21ef9585e87b45a281701a0059274306",
            "ca97f1da0e114bd3acaa5f445a5a5f48",
            "8db05f7c6d75460d870d04283a30545c",
            "f077685de662454c94e27a2234cbab75",
            "de5f77ed74fa472d8b9c0f23e6c0d3b2",
            "30c38c2e732443d1bfafd310ce983e31",
            "cb1058f2662d44e48f2a110d2c1509d1",
            "c6f52d8c98ed4525a3789d249aa9bb52",
            "5e0423629acd4f78bd19e2c77ccd9744",
            "a5910ef51fa44351a3f6af2797a47a0b",
            "054feda94be54dc4970371b4baa22162",
            "b106b5372a2c4442a762d95df5d28447",
            "d4d940d741c94a74ab797d60649d01c5",
            "32e375ece077435197b98173b0d6b947",
            "91f4b879ded347f797acb8af7075e9d9",
            "47581d78c15a4046b376ca66a64dca83",
            "489261ae293f452a931f1f7d20dbdacf",
            "810b24b251a9435cbc6aff3d4dc2d08b",
            "7cee59589c4348d2acf2b1440a8cf837",
            "bd05dc46c49f4cf8808a3d614cc608d1",
            "be669f74d6994bc2bddb407ba724597a",
            "562ca9c981c14f988326ac203be5160f",
            "cecc7ebaced542a793bc4922236e618e",
            "6f1ea1985457489d827da2aa5ed08e5c",
            "52b934ada68d489db4c1b5fd74b802ff"
          ]
        },
        "id": "_wnS-B_CFlej",
        "outputId": "a5db9a25-93ea-48a5-9cb5-934d96884592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0ad51dcee7140c89d2ded5cea02bb97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dc2a3d59da94d448308ad938f703c98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd338cccbc9a474b859994a8285b3be7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f077685de662454c94e27a2234cbab75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91f4b879ded347f797acb8af7075e9d9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"squad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ppt_pzquvVR6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V_Zp2qTQIdwk"
      },
      "outputs": [],
      "source": [
        "train_data = dataset[\"train\"]\n",
        "val_data = dataset[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLGdTiF9J-Ug",
        "outputId": "6ae9d30d-fd54-427f-f218-fcd95ec9f99e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID:\n",
            "5733be284776f41900661182\n",
            "\n",
            "TITLE:\n",
            "University_of_Notre_Dame\n",
            "\n",
            "CONTEXT:\n",
            "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
            "\n",
            "QUESTION:\n",
            "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "\n",
            "ANSWERS:\n",
            "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print the first sample\n",
        "sample = train_data[0]\n",
        "for key, value in sample.items():\n",
        "    print(f\"{key.upper()}:\\n{value}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-eSq3XzKpPY",
        "outputId": "bc0f5ad4-97c9-4a9b-eeb2-93fa2867274f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER TEXT: Saint Bernadette Soubirous\n",
            "EXTRACTED: Saint Bernadette Soubirous\n",
            "Match: True\n"
          ]
        }
      ],
      "source": [
        "context = sample[\"context\"]\n",
        "answer_text = sample[\"answers\"][\"text\"][0]\n",
        "answer_start = sample[\"answers\"][\"answer_start\"][0]\n",
        "\n",
        "# Extract substring from context and compare\n",
        "extracted_text = context[answer_start : answer_start + len(answer_text)]\n",
        "\n",
        "print(\"ANSWER TEXT:\", answer_text)\n",
        "print(\"EXTRACTED:\", extracted_text)\n",
        "print(\"Match:\", answer_text == extracted_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnPc7pCKKsab",
        "outputId": "aa018034-a26b-4348-9e0a-1340383af172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique topics in training set: 442\n"
          ]
        }
      ],
      "source": [
        "unique_titles = set([sample[\"title\"] for sample in train_data])\n",
        "print(f\"Unique topics in training set: {len(unique_titles)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_3verfKKyMj",
        "outputId": "434cb635-a25e-466f-fe39-dc9019c7a4c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 87599})\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "answer_counts = Counter([len(sample[\"answers\"][\"text\"]) for sample in train_data])\n",
        "print(answer_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0kMwgFgwNivg"
      },
      "outputs": [],
      "source": [
        "#Basic Tokenizor\n",
        "def simple_tokenizer(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
        "    return text.split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nvTBnLUKoBFC"
      },
      "outputs": [],
      "source": [
        "def build_vocab(data, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for sample in data:\n",
        "        tokens = simple_tokenizer(sample[\"context\"]) + simple_tokenizer(sample[\"question\"])\n",
        "        counter.update(tokens)\n",
        "\n",
        "    vocab = {\"[PAD]\": 0, \"[UNK]\": 1, \"[CLS]\": 2, \"[SEP]\": 3}\n",
        "    index = 4\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s_qmRYpCodGH"
      },
      "outputs": [],
      "source": [
        "def tokens_to_ids(tokens, vocab):\n",
        "    return [vocab.get(token, vocab[\"[UNK]\"]) for token in tokens]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XxgGZsmLpFbk"
      },
      "outputs": [],
      "source": [
        "def encode_sample(sample, vocab, max_len=384):\n",
        "    question = sample[\"question\"]\n",
        "    context = sample[\"context\"]\n",
        "    answer_text = sample[\"answers\"][\"text\"][0]\n",
        "    answer_start_char = sample[\"answers\"][\"answer_start\"][0]\n",
        "\n",
        "    question_tokens = simple_tokenizer(question)\n",
        "    context_tokens = simple_tokenizer(context)\n",
        "\n",
        "    # Combine and add special tokens\n",
        "    input_tokens = [\"[CLS]\"] + question_tokens + [\"[SEP]\"] + context_tokens + [\"[SEP]\"]\n",
        "    token_ids = tokens_to_ids(input_tokens, vocab)\n",
        "    attention_mask = [1] * len(token_ids)\n",
        "\n",
        "    # Padding\n",
        "    pad_len = max_len - len(token_ids)\n",
        "    if pad_len > 0:\n",
        "        token_ids += [vocab[\"[PAD]\"]] * pad_len\n",
        "        attention_mask += [0] * pad_len\n",
        "    else:\n",
        "        token_ids = token_ids[:max_len]\n",
        "        attention_mask = attention_mask[:max_len]\n",
        "\n",
        "    # Approximate start/end positions in token space\n",
        "    answer_start_words = len(simple_tokenizer(context[:answer_start_char]))\n",
        "    answer_end_words = len(simple_tokenizer(context[:answer_start_char + len(answer_text)])) - 1\n",
        "\n",
        "    context_start_idx = len(question_tokens) + 2\n",
        "    start_pos = min(context_start_idx + answer_start_words, max_len - 1)\n",
        "    end_pos = min(context_start_idx + answer_end_words, max_len - 1)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": token_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"start_position\": start_pos,\n",
        "        \"end_position\": end_pos,\n",
        "        \"tokens\": input_tokens\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7reDlpYqpIOd"
      },
      "outputs": [],
      "source": [
        "# encoded_train = [encode_sample(sample, vocab) for sample in train_data.select(range(1000))]\n",
        "encoded_train = [encode_sample(sample, vocab) for sample in train_data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tY7R90F1p4nV",
        "outputId": "305a7453-2a11-4f3d-c6b8-d68837d8a008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['[CLS]', 'to', 'whom', 'did', 'the', 'virgin', 'mary', 'allegedly', 'appear', 'in', '1858', 'in', 'lourdes', 'france', '[SEP]', 'architecturally', 'the', 'school', 'has', 'a', 'catholic', 'character', 'atop', 'the', 'main', 'buildings', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', 'immediately', 'in', 'front', 'of']\n",
            "Start Pos: 105\n",
            "End Pos: 107\n",
            "Answer Tokens: ['saint', 'bernadette', 'soubirous']\n"
          ]
        }
      ],
      "source": [
        "sample = encoded_train[0]\n",
        "print(\"Tokens:\", sample[\"tokens\"][:40])\n",
        "print(\"Start Pos:\", sample[\"start_position\"])\n",
        "print(\"End Pos:\", sample[\"end_position\"])\n",
        "print(\"Answer Tokens:\", sample[\"tokens\"][sample[\"start_position\"]:sample[\"end_position\"]+1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwpYWrm1p7tv"
      },
      "outputs": [],
      "source": [
        "#Hello\n",
        "\n",
        "#Transformer Buildings and Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OvAXKu330pAp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eUsujKaL0rBj"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
        "        return self.pos_embed(positions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HEX3OTtm0tjL"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Reshape for heads\n",
        "        def reshape(x):  # (B, S, D) → (B, H, S, D//H)\n",
        "            return x.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        q = reshape(q)\n",
        "        k = reshape(k)\n",
        "        v = reshape(v)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask[:, None, None, :] == 0, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attn, v)  # (B, H, S, D//H)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "        return self.out_proj(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LMzUnczt0vkQ"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, ff_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(F.relu(self.linear1(x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "STbcNsp50xwq"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model, ff_dim)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_out = self.attn(x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + self.dropout(ff_out))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lDk1VUuq00mE"
      },
      "outputs": [],
      "source": [
        "class LearnGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=8, ff_dim=512, num_layers=4, max_len=384):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = PositionalEncoding(max_len, d_model)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.qa_outputs = nn.Linear(d_model, 2)  # for start and end logits\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        x = self.token_embed(input_ids) + self.pos_embed(input_ids)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, attention_mask)\n",
        "\n",
        "        logits = self.qa_outputs(x)  # (B, S, 2)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksH1wbN09O9r"
      },
      "outputs": [],
      "source": [
        "class LearnGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=8, ff_dim=512, num_layers=4, max_len=384):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = PositionalEncoding(max_len, d_model)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.qa_outputs = nn.Linear(d_model, 2)  # for start and end logits\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        x = self.token_embed(input_ids) + self.pos_embed(input_ids)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, attention_mask)\n",
        "\n",
        "        logits = self.qa_outputs(x)  # (B, S, 2)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a106Zcpf9Qc7"
      },
      "outputs": [],
      "source": [
        "class LearnGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=768, num_heads=8, ff_dim=512, num_layers=4, max_len=384):\n",
        "        super().__init__()\n",
        "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embed = PositionalEncoding(max_len, d_model)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, ff_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.qa_outputs = nn.Linear(d_model, 2)  # for start and end logits\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        x = self.token_embed(input_ids) + self.pos_embed(input_ids)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, attention_mask)\n",
        "\n",
        "        logits = self.qa_outputs(x)  # (B, S, 2)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GNyHulFm02_4"
      },
      "outputs": [],
      "source": [
        "def compute_loss(start_logits, end_logits, start_pos, end_pos):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    loss_start = loss_fct(start_logits, start_pos)\n",
        "    loss_end = loss_fct(end_logits, end_pos)\n",
        "    return (loss_start + loss_end) / 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pF45RcsC054Y"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(vocab)\n",
        "model = LearnGPT(vocab_size)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTU4LXzF1N1u"
      },
      "outputs": [],
      "source": [
        "#PHASE 4 TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mltMUriR07yS"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw9swWwP2_Mf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch]),\n",
        "        \"start_positions\": torch.tensor([x[\"start_position\"] for x in batch]),\n",
        "        \"end_positions\": torch.tensor([x[\"end_position\"] for x in batch])\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(encoded_train, batch_size=8, shuffle=True, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-AVY333y1Q5u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "7bd5061c-eab8-44fa-ebb9-0a29c2815677"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-e5b4a34a59c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnum_training_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m lr_scheduler = get_scheduler(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 10\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "q_0Cj-bQ4ooW"
      },
      "outputs": [],
      "source": [
        "best_val_loss = float('inf')  # Initialize with a very high value\n",
        "patience = 3  # Number of epochs to wait before stopping\n",
        "epochs_without_improvement = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjCyWf4n4zm-",
        "outputId": "a5f57838-da55-4df4-8755-8030d1249a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [01:14<00:00, 147.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 4.3537\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [01:13<00:00, 148.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 4.2335\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 4208/10950 [00:26<00:42, 158.20it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[57], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m end_pos \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_positions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 17\u001b[0m start_logits, end_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(start_logits, end_logits, start_pos, end_pos)\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32md:\\K214578\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\K214578\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[48], line 17\u001b[0m, in \u001b[0;36mLearnGPT.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embed(input_ids) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(input_ids)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[1;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(x)  \u001b[38;5;66;03m# (B, S, 2)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m start_logits, end_logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "File \u001b[1;32md:\\K214578\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\K214578\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[45], line 11\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 11\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_out))\n\u001b[0;32m     13\u001b[0m     ff_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(x)\n",
            "File \u001b[1;32md:\\K214578\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\K214578\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[43], line 13\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     12\u001b[0m     batch_size, seq_len, d_model \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m---> 13\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Reshape for heads\u001b[39;00m\n",
            "File \u001b[1;32md:\\K214578\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32md:\\K214578\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32md:\\K214578\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_pos = batch['start_positions'].to(device)\n",
        "        end_pos = batch['end_positions'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        start_logits, end_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = compute_loss(start_logits, end_logits, start_pos, end_pos)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if 'lr_scheduler' in locals():\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kgfqF7Ib5t4c"
      },
      "outputs": [],
      "source": [
        "def create_val_loader(val_data, vocab, batch_size=8):\n",
        "  encoded_val = [encode_sample(sample, vocab) for sample in val_data]\n",
        "  val_loader = DataLoader(encoded_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "  return val_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rbORH8Wa5-E7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "1cde2f9d-d4ff-4f87-ce58-463281657d4c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DataLoader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c5b49eeb77aa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_val_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-2d2cedd48068>\u001b[0m in \u001b[0;36mcreate_val_loader\u001b[0;34m(val_data, vocab, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_val_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mencoded_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
          ]
        }
      ],
      "source": [
        "val_loader = create_val_loader(val_data, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "7Ma87Wid6CWL",
        "outputId": "9a0ca002-3f36-4c07-f5e1-464bfd4368ac"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'val_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-37e91b203b7a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'val_loader' is not defined"
          ]
        }
      ],
      "source": [
        "print(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fs7-anvc1S_q",
        "outputId": "e83e3dbe-c977-4570-90fb-53af5589bcdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [07:38<00:00, 23.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 4.3781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1322/1322 [00:12<00:00, 102.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Validation Loss: 4.3330\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [07:39<00:00, 23.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 4.1779\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1322/1322 [00:12<00:00, 103.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Validation Loss: 4.2278\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [07:37<00:00, 23.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 3.9813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1322/1322 [00:12<00:00, 103.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Validation Loss: 4.1231\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [07:42<00:00, 23.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 3.8042\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1322/1322 [00:12<00:00, 103.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Validation Loss: 4.1058\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [07:36<00:00, 23.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Loss: 3.6560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1322/1322 [00:12<00:00, 102.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Validation Loss: 4.1489\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [07:38<00:00, 23.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Loss: 3.5210\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1322/1322 [00:12<00:00, 102.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Validation Loss: 4.1631\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [07:38<00:00, 23.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Loss: 3.4002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1322/1322 [00:12<00:00, 103.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Validation Loss: 4.2845\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float('inf')\n",
        "patience = 3\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_pos = batch['start_positions'].to(device)\n",
        "        end_pos = batch['end_positions'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        start_logits, end_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = compute_loss(start_logits, end_logits, start_pos, end_pos)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if 'lr_scheduler' in locals():\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            start_pos = batch['start_positions'].to(device)\n",
        "            end_pos = batch['end_positions'].to(device)\n",
        "\n",
        "            start_logits, end_logits = model(input_ids, attention_mask)\n",
        "            loss = compute_loss(start_logits, end_logits, start_pos, end_pos)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "QpZo27ZMBM60"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "A-L6KuS55OaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a829bbc-f83d-467e-bdc2-98be3e393df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Evaluation + Testing\n",
        "import torch\n",
        "\n",
        "vocab_size = len(vocab)  # Assuming 'vocab' is defined from your training\n",
        "model = LearnGPT(vocab_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"LGM.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ib2sj00a5Qh8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Get a random index within the range of the dataset\n",
        "random_index = random.randint(0, len(train_data) - 1)\n",
        "\n",
        "# Select the sample using the random index\n",
        "sample = train_data[random_index]\n",
        "  # any sample you like #3046 #2047 #9067 #3400 #3500 #3800 #4000 #5000\n",
        "encoded = encode_sample(sample, vocab)\n",
        "\n",
        "input_ids = torch.tensor([encoded[\"input_ids\"]]).to(device)\n",
        "attention_mask = torch.tensor([encoded[\"attention_mask\"]]).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "2az4loy_5Sm2"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    start_logits, end_logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "end_idx = torch.argmax(end_logits, dim=1).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Coy6C_ll5U0P",
        "outputId": "13366ded-8294-41ee-8e7f-7ad0fe50d87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What was one of the first alloys used by humans?\n",
            "True Answer: meteoric iron\n",
            "Predicted Answer: iron meteorites which occasionally fall down on earth from outer space as no metallurgic processes were used to separate iron from nickel\n"
          ]
        }
      ],
      "source": [
        "tokens = encoded[\"tokens\"]  # This is your `[CLS] question [SEP] context [SEP]` token list\n",
        "\n",
        "# Fix case where model predicts end < start    1078  99  32\n",
        "if end_idx < start_idx:\n",
        "    end_idx = start_idx\n",
        "\n",
        "predicted_tokens = tokens[start_idx:end_idx+1]\n",
        "predicted_answer = \" \".join(predicted_tokens).replace(\" ##\", \"\")\n",
        "\n",
        "print(\"Question:\", sample[\"question\"])\n",
        "print(\"True Answer:\", sample[\"answers\"][\"text\"][0])\n",
        "print(\"Predicted Answer:\", predicted_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogxxucBy5-wZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgfllpOQXKWZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI9MOIRfXCw7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Save model\n",
        "model_path = \"LGM.pt\"\n",
        "torch.save(model.state_dict(), model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UdU0lBc85-tB",
        "outputId": "8b88742b-30e3-4c46-e857-2a56fa912594"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[0;32m      2\u001b[0m files\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGM.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"LGM.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ6ANbkRXJLB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizerFast\n",
        "\n",
        "# Load tokenizer (ensure it's the same as used during training)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define the model architecture (must match the saved model)\n",
        "class LearnGPT(torch.nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(LearnGPT, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.qa_outputs = torch.nn.Linear(self.bert.config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n",
        "\n",
        "# Load the saved model\n",
        "vocab_size = tokenizer.vocab_size  # Ensure vocab_size matches training\n",
        "model = LearnGPT(vocab_size)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device)) # Load the model's state dictionary\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSnI3ucoXJDf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss', marker='o')\n",
        "plt.plot(val_losses, label='Validation Loss', marker='o')\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.xticks(range(len(train_losses)), range(1, len(train_losses) + 1))  # Set x-axis ticks to represent epochs\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amm2d10WXI5d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNkgIcX0AvRo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3Kj-ZRHAvRo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv_NbZJ55-qx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def exact_match(predicted, true):\n",
        "    # Exact match = if they are exactly equal\n",
        "    return int(predicted == true)\n",
        "\n",
        "def f1(predicted, true):\n",
        "    # Convert answers to token sets for F1 calculation\n",
        "    pred_tokens = set(predicted.split())\n",
        "    true_tokens = set(true.split())\n",
        "\n",
        "    # Calculate overlap\n",
        "    intersection = len(pred_tokens.intersection(true_tokens))\n",
        "    if intersection == 0:\n",
        "        return 0\n",
        "    precision = intersection / len(pred_tokens)\n",
        "    recall = intersection / len(true_tokens)\n",
        "    return 2 * (precision * recall) / (precision + recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwcCCSmd6BXd"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_em = 0\n",
        "    total_f1 = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            start_positions = batch[\"start_positions\"].to(device)\n",
        "            end_positions = batch[\"end_positions\"].to(device)\n",
        "\n",
        "            start_logits, end_logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Get predictions\n",
        "            start_idx = torch.argmax(start_logits, dim=1)\n",
        "            end_idx = torch.argmax(end_logits, dim=1)\n",
        "\n",
        "            # Convert token indices back to string answers\n",
        "            for i in range(len(start_idx)):\n",
        "                start = start_idx[i].item()\n",
        "                end = end_idx[i].item()\n",
        "\n",
        "                # Handle case where end < start\n",
        "                if end < start:\n",
        "                    end = start\n",
        "\n",
        "                predicted_tokens = batch[\"tokens\"][i][start:end+1]\n",
        "                predicted_answer = \" \".join(predicted_tokens).replace(\" ##\", \"\")\n",
        "                true_answer = batch[\"answers\"][\"text\"][i]\n",
        "\n",
        "                # Calculate Exact Match and F1\n",
        "                total_em += exact_match(predicted_answer, true_answer)\n",
        "                total_f1 += f1(predicted_answer, true_answer)\n",
        "                total_samples += 1\n",
        "\n",
        "    # Compute average scores\n",
        "    em_score = total_em / total_samples\n",
        "    f1_score_avg = total_f1 / total_samples\n",
        "\n",
        "    return em_score, f1_score_avg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJPQ_-c26OU-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Assuming you've already defined `encode_sample` and `train_data` above.\n",
        "\n",
        "# Step 1: Define a custom dataset class for the validation set\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, data, vocab, max_len=384):\n",
        "        self.data = data\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        return encode_sample(sample, self.vocab, self.max_len)\n",
        "\n",
        "# Step 2: Prepare validation data (using the validation split of the dataset)\n",
        "val_data = dataset[\"validation\"]  # Assuming dataset is already loaded with train/validation splits\n",
        "\n",
        "# Step 3: Tokenize the validation dataset (using encode_sample)\n",
        "encoded_val = [encode_sample(sample, vocab) for sample in val_data]\n",
        "\n",
        "# Step 4: Convert to PyTorch Dataset and DataLoader\n",
        "val_dataset = QADataset(encoded_val, vocab)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Step 5: Now `val_loader` is ready for use during evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOFqInSc6gJi",
        "outputId": "f67078e5-6a1c-430e-eba9-2c6d4ef909c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': '56be4db0acb8001400a502ec', 'title': 'Super_Bowl_50', 'context': 'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.', 'question': 'Which NFL team represented the AFC at Super Bowl 50?', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}\n"
          ]
        }
      ],
      "source": [
        "# Print one sample from val_data\n",
        "print(val_data[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "YVzhRI5m6_-g",
        "outputId": "ced5c60c-0940-4ea4-d64e-b2052a7afa72"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "The table can't have duplicated columns but columns ['question'] are duplicated.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ad298843ff78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add the 'question' column to val_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_trimmed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m         }\n\u001b[1;32m    556\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# Call actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36madd_column\u001b[0;34m(self, name, column, new_fingerprint, feature)\u001b[0m\n\u001b[1;32m   5797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5798\u001b[0m         \u001b[0mcolumn_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInMemoryTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpyarrow_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5799\u001b[0;31m         \u001b[0m_check_column_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcolumn_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5800\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5801\u001b[0m         \u001b[0;31m# Concatenate tables horizontally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_check_column_names\u001b[0;34m(column_names)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mduplicated_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The table can't have duplicated columns but columns {duplicated_columns} are duplicated.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The table can't have duplicated columns but columns ['question'] are duplicated."
          ]
        }
      ],
      "source": [
        "# Trim train_data to match the size of val_data\n",
        "train_data_trimmed = train_data.select(range(len(val_data)))\n",
        "\n",
        "# Add the 'question' column to val_data\n",
        "val_data = val_data.add_column('question', [sample['question'] for sample in train_data_trimmed])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "LOEu8CqA611y",
        "outputId": "425cd5e1-a5ca-4c8d-aeae-23fcc2d62b70"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Training and validation data lengths do not match!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-5d9c4fd8b3a1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ensure that train_data and val_data are the same length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Training and validation data lengths do not match!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add the 'question' column from train_data to val_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Training and validation data lengths do not match!"
          ]
        }
      ],
      "source": [
        "# Ensure that train_data and val_data are the same length\n",
        "assert len(train_data) == len(val_data), \"Training and validation data lengths do not match!\"\n",
        "\n",
        "# Add the 'question' column from train_data to val_data\n",
        "val_data = val_data.add_column('question', [sample['question'] for sample in train_data])\n",
        "\n",
        "# Now you should be able to access 'question' in val_data as well\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "uDHsLVuy6EIg",
        "outputId": "f059d9f8-bcbb-404b-ebb2-2891d9862384"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'Dataset' object has no attribute 'eval'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-7c49336801c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming `val_loader` is your validation data loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Exact Match (EM): {em:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"F1 Score: {f1_avg:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-2e60869857df>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtotal_em\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtotal_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'eval'"
          ]
        }
      ],
      "source": [
        "# Assuming `val_loader` is your validation data loader\n",
        "em, f1_avg = evaluate_model(model, val_loader, device)\n",
        "\n",
        "print(f\"Exact Match (EM): {em:.4f}\")\n",
        "print(f\"F1 Score: {f1_avg:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZz6Hfdh2bED"
      },
      "source": [
        "### visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asNghktJ2d2D"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUWt7cJl3tO_"
      },
      "outputs": [],
      "source": [
        "def calculate_f1(start_preds, end_preds, start_positions, end_positions):\n",
        "    \"\"\"Calculates F1 score for question answering.\"\"\"\n",
        "    # Convert predictions and targets to lists\n",
        "    start_preds = start_preds.tolist()\n",
        "    end_preds = end_preds.tolist()\n",
        "    start_positions = start_positions.tolist()\n",
        "    end_positions = end_positions.tolist()\n",
        "\n",
        "    f1_scores = []\n",
        "    for i in range(len(start_preds)):\n",
        "        pred_start = start_preds[i]\n",
        "        pred_end = end_preds[i]\n",
        "        true_start = start_positions[i]\n",
        "        true_end = end_positions[i]\n",
        "\n",
        "        # Handle cases where pred_end < pred_start\n",
        "        if pred_end < pred_start:\n",
        "            pred_end = pred_start\n",
        "\n",
        "        # Calculate F1 score for this example\n",
        "        pred_span = set(range(pred_start, pred_end + 1))\n",
        "        true_span = set(range(true_start, true_end + 1))\n",
        "        overlap = len(pred_span.intersection(true_span))\n",
        "        if overlap == 0:\n",
        "            f1_scores.append(0.0)\n",
        "        else:\n",
        "            precision = overlap / len(pred_span)\n",
        "            recall = overlap / len(true_span)\n",
        "            f1 = (2 * precision * recall) / (precision + recall)\n",
        "            f1_scores.append(f1)\n",
        "\n",
        "    # Return average F1 score\n",
        "    return sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "\n",
        "def visualize_loss(losses):\n",
        "    \"\"\"Visualizes training loss over epochs.\"\"\"\n",
        "    plt.plot(losses)\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Olh-y_eq38iX",
        "outputId": "4e1a4316-bda1-4688-e9e4-a2a3f8fc87db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10950/10950 [00:52<00:00, 208.03it/s]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'tolist'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-5fbb2267280e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Calculate F1 score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     f1 = calculate_f1(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mall_start_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_end_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_start_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_end_positions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-153-891b90e98bbd>\u001b[0m in \u001b[0;36mcalculate_f1\u001b[0;34m(start_preds, end_preds, start_positions, end_positions)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Calculates F1 score for question answering.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Convert predictions and targets to lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstart_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mend_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
          ]
        }
      ],
      "source": [
        "# ... (your existing training loop) ...\n",
        "\n",
        "epoch_losses = []\n",
        "epoch_f1_scores = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ... (rest of your training loop) ...\n",
        "\n",
        "    # Evaluation at the end of each epoch\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_start_preds = []\n",
        "    all_end_preds = []\n",
        "    all_start_positions = []\n",
        "    all_end_positions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(train_loader):  # or use a separate validation loader\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            start_pos = batch['start_positions'].to(device)\n",
        "            end_pos = batch['end_positions'].to(device)\n",
        "\n",
        "            start_logits, end_logits = model(input_ids, attention_mask)\n",
        "            loss = compute_loss(start_logits, end_logits, start_pos, end_pos)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predictions\n",
        "            start_preds = torch.argmax(start_logits, dim=1)\n",
        "            end_preds = torch.argmax(end_logits, dim=1)\n",
        "\n",
        "            # Store predictions and targets for F1 calculation\n",
        "            all_start_preds.extend(start_preds)\n",
        "            all_end_preds.extend(end_preds)\n",
        "            all_start_positions.extend(start_pos)\n",
        "            all_end_positions.extend(end_pos)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    epoch_losses.append(avg_loss)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = calculate_f1(\n",
        "        all_start_preds, all_end_preds, all_start_positions, all_end_positions\n",
        "    )\n",
        "    epoch_f1_scores.append(f1)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "# Visualize loss\n",
        "visualize_loss(epoch_losses)\n",
        "\n",
        "# Print or visualize F1 scores if needed\n",
        "print(\"F1 scores over epochs:\", epoch_f1_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3z_0PVLjB43"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pJV3Q8NjDdY"
      },
      "source": [
        "#Advanced Tokenization with BERT\n",
        "\n",
        "\n",
        "##Starting here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeYdXZ3AjCkB"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZY0mT_rlHg1"
      },
      "outputs": [],
      "source": [
        "def encode_sample_fast(sample, tokenizer, max_len=384):\n",
        "    question = sample[\"question\"]\n",
        "    context = sample[\"context\"]\n",
        "    answer_text = sample[\"answers\"][\"text\"][0]\n",
        "    answer_start = sample[\"answers\"][\"answer_start\"][0]\n",
        "\n",
        "    # Use tokenizer with offset mapping\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_offsets_mapping=True,\n",
        "        return_tensors=None  # returns dict of lists (not tensors)\n",
        "    )\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    offset_mapping = inputs[\"offset_mapping\"]\n",
        "\n",
        "    # Find start and end positions\n",
        "    start_char = answer_start\n",
        "    end_char = answer_start + len(answer_text)\n",
        "\n",
        "    start_token = end_token = 0\n",
        "    for idx, (start, end) in enumerate(offset_mapping):\n",
        "        if start <= start_char < end:\n",
        "            start_token = idx\n",
        "        if start < end_char <= end:\n",
        "            end_token = idx\n",
        "            break\n",
        "\n",
        "    # Handle edge case if token not found\n",
        "    if start_token == 0 and end_token == 0:\n",
        "        start_token = end_token = 0\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"start_position\": start_token,\n",
        "        \"end_position\": end_token\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjUVp0vBliKq"
      },
      "outputs": [],
      "source": [
        "encoded_train = [encode_sample_fast(sample, tokenizer) for sample in train_data]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2C9q5iwlkna"
      },
      "outputs": [],
      "source": [
        "encoded_val = [encode_sample_fast(sample, tokenizer) for sample in val_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65F6TaL7xTZD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MAX_LEN = 384\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "EMBED_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "FF_DIM = 512\n",
        "NUM_LAYERS = 8\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzHx5l0XqcdS"
      },
      "outputs": [],
      "source": [
        "class SquadDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.encodings[idx]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n",
        "            \"attention_mask\": torch.tensor(item[\"attention_mask\"]),\n",
        "            \"start_positions\": torch.tensor(item[\"start_position\"]),\n",
        "            \"end_positions\": torch.tensor(item[\"end_position\"]),\n",
        "        }\n",
        "\n",
        "train_dataset = SquadDataset(encoded_train)\n",
        "val_dataset = SquadDataset(encoded_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hu14veyx9pp"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)].to(x.device)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax9E04nYxdFZ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.fc_out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        out = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.fc_out(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrTXdqmlxg_o"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f6xbjZoxj6d"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.norm1(x + self.attn(x, mask))\n",
        "        x = self.norm2(x + self.ff(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObQIxhohxnDa"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len=512):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_enc = PositionalEncoding(embed_dim, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(embed_dim, num_heads, ff_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.embed(input_ids)\n",
        "        x = self.pos_enc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask.unsqueeze(1).unsqueeze(2))  # (B, 1, 1, T)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dufZ6plZxqf7"
      },
      "outputs": [],
      "source": [
        "class QAModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(vocab_size, embed_dim, num_heads, ff_dim, num_layers)\n",
        "        self.qa_outputs = nn.Linear(embed_dim, 2)  # for start and end\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        enc_output = self.encoder(input_ids, attention_mask)  # (B, T, D)\n",
        "        logits = self.qa_outputs(enc_output)  # (B, T, 2)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        return start_logits.squeeze(-1), end_logits.squeeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "xUfuqbYgxtpV",
        "outputId": "e5feb9e1-4e74-4553-f64c-cf35b3b2d323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Loss: 8.9202 - Time: 265.21s - ETA: 17m 40s\n",
            "Epoch 2 - Loss: 8.0496 - Time: 261.55s - ETA: 13m 10s\n",
            "Epoch 3 - Loss: 7.6936 - Time: 260.74s - ETA: 8m 45s\n",
            "Epoch 4 - Loss: 7.4471 - Time: 264.34s - ETA: 4m 22s\n",
            "Epoch 5 - Loss: 7.2138 - Time: 260.84s - ETA: 0m 0s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "model = QAModel(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, FF_DIM, NUM_LAYERS).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "epoch_times = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        start_positions = batch[\"start_positions\"].to(device)\n",
        "        end_positions = batch[\"end_positions\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        start_logits, end_logits = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = loss_fn(start_logits, start_positions) + loss_fn(end_logits, end_positions)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_duration = end_time - start_time\n",
        "    epoch_times.append(epoch_duration)\n",
        "\n",
        "    avg_epoch_time = sum(epoch_times) / len(epoch_times)\n",
        "    remaining_epochs = EPOCHS - (epoch + 1)\n",
        "    eta = avg_epoch_time * remaining_epochs\n",
        "\n",
        "    # Convert ETA to min:sec format\n",
        "    eta_min = int(eta // 60)\n",
        "    eta_sec = int(eta % 60)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(train_loader):.4f} - Time: {epoch_duration:.2f}s - ETA: {eta_min}m {eta_sec}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_c8ROGWAvR3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjaoVjaAEVZ5",
        "outputId": "0d283469-9404-41e3-d1d1-34c9b2c62fe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ],
      "source": [
        "!python -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "pMobEgo_7hjo",
        "outputId": "08cf8b8d-5e2d-4e26-95b5-22457d970297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📌 Question: Which NFL team represented the AFC at Super Bowl 50?\n",
            "✅ True Answer: Denver Broncos\n",
            "🤖 Predicted Answer: league ( nfl ) for the 2015 season. the american football conference ( afc ) champion denver broncos defeated the national football conference ( nfc ) champion carolina panthers 24 – 10 to earn their third super bowl title. the game was played on february 7, 2016, at levi ' s stadium in the san francisco bay area at santa clara, california. as this was the 50th super bowl, the league emphasized the \" golden anniversary \" with various gold - themed initiatives, as well as temporarily suspending the tradition of naming each super bowl game with roman numerals ( under which the game would have been known as \" super bowl l \" ), so that the logo could prominently feature the arabic numerals 50. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Pick a validation sample (or train sample for overfitting test)\n",
        "sample = val_data[0]  # Or train_data[0] to test overfitting\n",
        "\n",
        "# Encode the sample\n",
        "encoded = encode_sample_fast(sample, tokenizer)\n",
        "\n",
        "# Convert to tensors and move to device\n",
        "input_ids = torch.tensor(encoded[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "attention_mask = torch.tensor(encoded[\"attention_mask\"]).unsqueeze(0).to(device)\n",
        "\n",
        "# Run the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    start_logits, end_logits = model(input_ids, attention_mask)\n",
        "\n",
        "# Get predicted start/end positions\n",
        "start_idx = torch.argmax(start_logits, dim=1).item()\n",
        "end_idx = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "# Fix edge case: if end < start, set end = start\n",
        "if end_idx < start_idx:\n",
        "    end_idx = start_idx\n",
        "\n",
        "# Decode predicted tokens\n",
        "decoded_tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"][start_idx:end_idx + 1])\n",
        "predicted_answer = tokenizer.convert_tokens_to_string(decoded_tokens)\n",
        "\n",
        "# Print results\n",
        "print(\"📌 Question:\", sample[\"question\"])\n",
        "print(\"✅ True Answer:\", sample[\"answers\"][\"text\"][0])\n",
        "print(\"🤖 Predicted Answer:\", predicted_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M19pHFC-xf8i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWVj480kxaq0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Butk4P5AvG45"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = QADataset(encoded_train)\n",
        "val_dataset = QADataset(encoded_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2PoPBPYvPb0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b0ad51dcee7140c89d2ded5cea02bb97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cba4f75d60340d186b184b03e61b5cc",
              "IPY_MODEL_4be46c5d63b440be874bcabbd3aad777",
              "IPY_MODEL_18315fe95ea840ea9ad842f8e04f2fb9"
            ],
            "layout": "IPY_MODEL_87f47d083acb484c829de79191b29af3"
          }
        },
        "1cba4f75d60340d186b184b03e61b5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f648acc8ea74aadb24dd3680f28a63d",
            "placeholder": "​",
            "style": "IPY_MODEL_3df996eb470f43fd840f4e7ef7f8f423",
            "value": "README.md: 100%"
          }
        },
        "4be46c5d63b440be874bcabbd3aad777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8df689d6a61349f480c8ddbfc1b12202",
            "max": 7620,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16c882338d4f4facb47ddf1e4d5bef57",
            "value": 7620
          }
        },
        "18315fe95ea840ea9ad842f8e04f2fb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc30ad71e2c240abb319438d9c785168",
            "placeholder": "​",
            "style": "IPY_MODEL_374089da8a8b4913a5708db903875fe9",
            "value": " 7.62k/7.62k [00:00&lt;00:00, 145kB/s]"
          }
        },
        "87f47d083acb484c829de79191b29af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f648acc8ea74aadb24dd3680f28a63d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df996eb470f43fd840f4e7ef7f8f423": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8df689d6a61349f480c8ddbfc1b12202": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c882338d4f4facb47ddf1e4d5bef57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc30ad71e2c240abb319438d9c785168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "374089da8a8b4913a5708db903875fe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dc2a3d59da94d448308ad938f703c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e4b608a58bd4b44a528768126a42bbd",
              "IPY_MODEL_3aeec118061a4ed9b8932625d0309c20",
              "IPY_MODEL_e48547de62f54d07ba4e224d98a392da"
            ],
            "layout": "IPY_MODEL_37aec77be48146999c155177fa7e0caf"
          }
        },
        "1e4b608a58bd4b44a528768126a42bbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab134096846b446ca199d2334f469536",
            "placeholder": "​",
            "style": "IPY_MODEL_bead121e2d9d4a3db2532a4cfc24267b",
            "value": "train-00000-of-00001.parquet: 100%"
          }
        },
        "3aeec118061a4ed9b8932625d0309c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b7dd395345645edbd3f529ca2164a34",
            "max": 14458314,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8eaa770fb50f4717ad9b2b54f3e1e0a2",
            "value": 14458314
          }
        },
        "e48547de62f54d07ba4e224d98a392da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_941a8e51c9f644ae8ae4c6cb56e89bca",
            "placeholder": "​",
            "style": "IPY_MODEL_bbcdc9e04bdb470984ae31a7e1d157ae",
            "value": " 14.5M/14.5M [00:00&lt;00:00, 54.9MB/s]"
          }
        },
        "37aec77be48146999c155177fa7e0caf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab134096846b446ca199d2334f469536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bead121e2d9d4a3db2532a4cfc24267b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b7dd395345645edbd3f529ca2164a34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eaa770fb50f4717ad9b2b54f3e1e0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "941a8e51c9f644ae8ae4c6cb56e89bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbcdc9e04bdb470984ae31a7e1d157ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd338cccbc9a474b859994a8285b3be7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49fcee6879104634a9ef5938fd173594",
              "IPY_MODEL_9e6d8c267348463fab6540119a855c0d",
              "IPY_MODEL_532a5027a26d481796abed5beb7686de"
            ],
            "layout": "IPY_MODEL_1083f2467b0546e1bb3f767fe24e048f"
          }
        },
        "49fcee6879104634a9ef5938fd173594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e457105ae614e7c83ab381a00a3e187",
            "placeholder": "​",
            "style": "IPY_MODEL_57afb409d15c47739aee2906d51bed7c",
            "value": "validation-00000-of-00001.parquet: 100%"
          }
        },
        "9e6d8c267348463fab6540119a855c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e21dcddb5b2a451d81123ff333ac2dec",
            "max": 1819889,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21ef9585e87b45a281701a0059274306",
            "value": 1819889
          }
        },
        "532a5027a26d481796abed5beb7686de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca97f1da0e114bd3acaa5f445a5a5f48",
            "placeholder": "​",
            "style": "IPY_MODEL_8db05f7c6d75460d870d04283a30545c",
            "value": " 1.82M/1.82M [00:00&lt;00:00, 29.4MB/s]"
          }
        },
        "1083f2467b0546e1bb3f767fe24e048f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e457105ae614e7c83ab381a00a3e187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57afb409d15c47739aee2906d51bed7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e21dcddb5b2a451d81123ff333ac2dec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21ef9585e87b45a281701a0059274306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca97f1da0e114bd3acaa5f445a5a5f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8db05f7c6d75460d870d04283a30545c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f077685de662454c94e27a2234cbab75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de5f77ed74fa472d8b9c0f23e6c0d3b2",
              "IPY_MODEL_30c38c2e732443d1bfafd310ce983e31",
              "IPY_MODEL_cb1058f2662d44e48f2a110d2c1509d1"
            ],
            "layout": "IPY_MODEL_c6f52d8c98ed4525a3789d249aa9bb52"
          }
        },
        "de5f77ed74fa472d8b9c0f23e6c0d3b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e0423629acd4f78bd19e2c77ccd9744",
            "placeholder": "​",
            "style": "IPY_MODEL_a5910ef51fa44351a3f6af2797a47a0b",
            "value": "Generating train split: 100%"
          }
        },
        "30c38c2e732443d1bfafd310ce983e31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_054feda94be54dc4970371b4baa22162",
            "max": 87599,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b106b5372a2c4442a762d95df5d28447",
            "value": 87599
          }
        },
        "cb1058f2662d44e48f2a110d2c1509d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4d940d741c94a74ab797d60649d01c5",
            "placeholder": "​",
            "style": "IPY_MODEL_32e375ece077435197b98173b0d6b947",
            "value": " 87599/87599 [00:00&lt;00:00, 180693.86 examples/s]"
          }
        },
        "c6f52d8c98ed4525a3789d249aa9bb52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0423629acd4f78bd19e2c77ccd9744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5910ef51fa44351a3f6af2797a47a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "054feda94be54dc4970371b4baa22162": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b106b5372a2c4442a762d95df5d28447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4d940d741c94a74ab797d60649d01c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32e375ece077435197b98173b0d6b947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91f4b879ded347f797acb8af7075e9d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47581d78c15a4046b376ca66a64dca83",
              "IPY_MODEL_489261ae293f452a931f1f7d20dbdacf",
              "IPY_MODEL_810b24b251a9435cbc6aff3d4dc2d08b"
            ],
            "layout": "IPY_MODEL_7cee59589c4348d2acf2b1440a8cf837"
          }
        },
        "47581d78c15a4046b376ca66a64dca83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd05dc46c49f4cf8808a3d614cc608d1",
            "placeholder": "​",
            "style": "IPY_MODEL_be669f74d6994bc2bddb407ba724597a",
            "value": "Generating validation split: 100%"
          }
        },
        "489261ae293f452a931f1f7d20dbdacf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_562ca9c981c14f988326ac203be5160f",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cecc7ebaced542a793bc4922236e618e",
            "value": 10570
          }
        },
        "810b24b251a9435cbc6aff3d4dc2d08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f1ea1985457489d827da2aa5ed08e5c",
            "placeholder": "​",
            "style": "IPY_MODEL_52b934ada68d489db4c1b5fd74b802ff",
            "value": " 10570/10570 [00:00&lt;00:00, 69904.40 examples/s]"
          }
        },
        "7cee59589c4348d2acf2b1440a8cf837": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd05dc46c49f4cf8808a3d614cc608d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be669f74d6994bc2bddb407ba724597a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "562ca9c981c14f988326ac203be5160f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cecc7ebaced542a793bc4922236e618e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f1ea1985457489d827da2aa5ed08e5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52b934ada68d489db4c1b5fd74b802ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}